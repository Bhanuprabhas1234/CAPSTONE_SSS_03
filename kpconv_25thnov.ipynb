{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e296c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version      : 2.9.0+cu128\n",
      "cuda available     : True\n",
      "cuda version       : 12.8\n",
      "numpy version      : 2.2.6\n",
      "sklearn version    : 1.7.2\n",
      "open3d version     : 0.19.0\n",
      "plyfile            : OK\n",
      "\n",
      "DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL 3 â€” VERIFY PACKAGES (run AFTER kernel restart)\n",
    "\n",
    "import torch, numpy as np, sklearn\n",
    "import open3d as o3d\n",
    "from plyfile import PlyData\n",
    "\n",
    "print(\"torch version      :\", torch.__version__)\n",
    "print(\"cuda available     :\", torch.cuda.is_available())\n",
    "print(\"cuda version       :\", torch.version.cuda if torch.cuda.is_available() else None)\n",
    "print(\"numpy version      :\", np.__version__)\n",
    "print(\"sklearn version    :\", sklearn.__version__)\n",
    "print(\"open3d version     :\", o3d.__version__)\n",
    "print(\"plyfile            : OK\")\n",
    "\n",
    "# Set DEVICE for later training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\nDEVICE =\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a43b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches: 120 val batches: 40\n"
     ]
    }
   ],
   "source": [
    "# CELL 4 â€” PLY Dataset Loader (for your 3-class dataset)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from plyfile import PlyData\n",
    "\n",
    "DATA_DIR = Path(\"/home/ccbd/Desktop/SSS_03/Data/train_sphere_ascii_roi\")\n",
    "\n",
    "label_map = {1: 0, 3: 1, 9: 2}   # final correct mapping\n",
    "\n",
    "def read_ply_hungary(path):\n",
    "    pd = PlyData.read(str(path))\n",
    "    v = pd[\"vertex\"].data\n",
    "\n",
    "    pts = np.vstack([v[\"x\"], v[\"y\"], v[\"z\"]]).T.astype(np.float32)\n",
    "    labels = np.array(v[\"scalar_NewClassification\"]).astype(np.int64)\n",
    "\n",
    "    # remap labels\n",
    "    labels = np.vectorize(lambda x: label_map.get(int(x), 255))(labels)\n",
    "    mask = labels != 255\n",
    "    return pts[mask], labels[mask]\n",
    "\n",
    "class HungaryPLYDataset(Dataset):\n",
    "    def __init__(self, root, files, points_per_sample=2048, augment=True):\n",
    "        self.root = Path(root)\n",
    "        self.files = files\n",
    "        self.points_per_sample = points_per_sample\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) * 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ply = self.files[np.random.randint(len(self.files))]\n",
    "        pts, labels = read_ply_hungary(self.root / ply)\n",
    "\n",
    "        N = len(pts)\n",
    "        P = self.points_per_sample\n",
    "        idxs = np.random.choice(N, P, replace=(N < P))\n",
    "\n",
    "        pts = pts[idxs]\n",
    "        labels = labels[idxs]\n",
    "\n",
    "        if self.augment:\n",
    "            theta = np.random.uniform(0, 2 * np.pi)\n",
    "            R = np.array([\n",
    "                [np.cos(theta), -np.sin(theta), 0],\n",
    "                [np.sin(theta),  np.cos(theta), 0],\n",
    "                [0, 0, 1]\n",
    "            ], dtype=np.float32)\n",
    "            pts = pts @ R.T\n",
    "            pts += np.random.normal(0, 0.01, pts.shape)\n",
    "\n",
    "        pts -= pts.mean(0, keepdims=True)\n",
    "\n",
    "        return torch.from_numpy(pts), torch.from_numpy(labels)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    pts = torch.stack([b[0] for b in batch], dim=0)\n",
    "    lbl = torch.stack([b[1] for b in batch], dim=0)\n",
    "    return pts, lbl\n",
    "\n",
    "# Create train/val split\n",
    "all_files = sorted([f.name for f in DATA_DIR.glob(\"*.ply\")])\n",
    "import random\n",
    "random.shuffle(all_files)\n",
    "N = len(all_files)\n",
    "train_files = all_files[:int(0.8*N)]\n",
    "val_files = all_files[int(0.8*N):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ds = HungaryPLYDataset(DATA_DIR, train_files, points_per_sample=1024, augment=True)\n",
    "val_ds   = HungaryPLYDataset(DATA_DIR, val_files,   points_per_sample=1024, augment=False)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True,\n",
    "                          num_workers=0, collate_fn=collate_batch, pin_memory=True)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
    "                          num_workers=0, collate_fn=collate_batch, pin_memory=True)\n",
    "print(\"train batches:\", len(train_loader), \"val batches:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f789816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KPNet(\n",
       "  (fc0): Linear(in_features=3, out_features=8, bias=True)\n",
       "  (kp1): KPConvLayer(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (kp2): KPConvLayer(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=19, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 5 â€” FIXED KPConv-Like Model (Dimension-Safe)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def knn(pts, K=8):\n",
    "    \"\"\"\n",
    "    pts: (B, P, 3)\n",
    "    returns idx: (B, P, K)\n",
    "    \"\"\"\n",
    "    dist = torch.cdist(pts, pts)  # (B, P, P)\n",
    "    idx = dist.topk(K, largest=False)[1]  # (B, P, K)\n",
    "    return idx\n",
    "\n",
    "class KPConvLayer(nn.Module):\n",
    "    def __init__(self, in_c, out_c, K=8):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_c + 3, out_c),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_c, out_c)\n",
    "        )\n",
    "\n",
    "    def forward(self, pts, feats):\n",
    "        \"\"\"\n",
    "        pts:   (B, P, 3)\n",
    "        feats: (B, P, C)\n",
    "        output: (B, P, out_c)\n",
    "        \"\"\"\n",
    "        B, P, C = feats.shape\n",
    "        K = self.K\n",
    "\n",
    "        idx = knn(pts, K)         # (B, P, K)\n",
    "\n",
    "        # gather neighbor coords: (B, P, K, 3)\n",
    "        pts_expand = pts.unsqueeze(2).expand(B, P, K, 3)\n",
    "        neigh_pts = torch.gather(pts.unsqueeze(1).expand(B, P, P, 3), \n",
    "                                 2, \n",
    "                                 idx.unsqueeze(-1).expand(B, P, K, 3))\n",
    "\n",
    "        # gather neighbor feats: (B, P, K, C)\n",
    "        feats_expand = feats.unsqueeze(1).expand(B, P, P, C)\n",
    "        neigh_feats = torch.gather(feats_expand,\n",
    "                                   2,\n",
    "                                   idx.unsqueeze(-1).expand(B, P, K, C))\n",
    "\n",
    "        # compute relative positions\n",
    "        rel = neigh_pts - pts_expand  # (B,P,K,3)\n",
    "\n",
    "        # concatenate features: (B,P,K, 3+C)\n",
    "        inp = torch.cat([rel, neigh_feats], dim=-1)\n",
    "\n",
    "        # MLP â†’ (B,P,K,out_c)\n",
    "        out = self.mlp(inp)\n",
    "\n",
    "        # Max over K neighbors â†’ (B,P,out_c)\n",
    "        out = out.max(dim=2)[0]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class KPNet(nn.Module):\n",
    "    def __init__(self, base=16, num_classes=3, K=8):\n",
    "        super().__init__()\n",
    "        self.fc0 = nn.Linear(3, base)\n",
    "\n",
    "        self.kp1 = KPConvLayer(base, base*2, K)\n",
    "        self.kp2 = KPConvLayer(base*2, base*4, K)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(base*4, base*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base*4, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, pts):\n",
    "        feats = F.relu(self.fc0(pts))\n",
    "        feats = self.kp1(pts, feats)\n",
    "        feats = self.kp2(pts, feats)\n",
    "        out = self.head(feats)\n",
    "        return out\n",
    "\n",
    "model = KPNet(base=8, num_classes=3, K=6).to(DEVICE)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868d1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "label_map = {1: 0, 3: 1, 9: 2}   # same mapping\n",
    "\n",
    "def read_ply_xyz_labels(path):\n",
    "    pd = PlyData.read(str(path))\n",
    "    v  = pd[\"vertex\"].data\n",
    "\n",
    "    xyz = np.vstack([v[\"x\"], v[\"y\"], v[\"z\"]]).T.astype(np.float32)\n",
    "    raw = np.array(v[\"scalar_NewClassification\"]).astype(np.int64)\n",
    "\n",
    "    lbl = np.vectorize(lambda x: label_map.get(int(x), 255))(raw)\n",
    "    mask = lbl != 255\n",
    "\n",
    "    xyz = xyz[mask]\n",
    "    lbl = lbl[mask]\n",
    "    return xyz, lbl\n",
    "\n",
    "\n",
    "class FullCloudChunkDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Takes each PLY, splits into sequential chunks of `points_per_chunk`.\n",
    "    - Caches each PLY in RAM (no re-reading)\n",
    "    - Optionally limits total chunks (max_chunks) to keep training fast\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, files, points_per_chunk=2048, augment=True, max_chunks=1500):\n",
    "        self.root = Path(root)\n",
    "        self.files = files\n",
    "        self.points_per_chunk = points_per_chunk\n",
    "        self.augment = augment\n",
    "\n",
    "        # 1) Cache clouds in memory\n",
    "        self.clouds = {}   # path -> (xyz, lbl)\n",
    "        for f in self.files:\n",
    "            path = self.root / f\n",
    "            xyz, lbl = read_ply_xyz_labels(path)\n",
    "            self.clouds[str(path)] = (xyz, lbl)\n",
    "\n",
    "        # 2) Build chunk index: (path_str, start_idx, end_idx)\n",
    "        chunks = []\n",
    "        for f in self.files:\n",
    "            path = self.root / f\n",
    "            xyz, lbl = self.clouds[str(path)]\n",
    "            N = xyz.shape[0]\n",
    "\n",
    "            for start in range(0, N, points_per_chunk):\n",
    "                end = min(start + points_per_chunk, N)\n",
    "                if end - start < points_per_chunk // 2:\n",
    "                    continue\n",
    "                chunks.append((str(path), start, end))\n",
    "\n",
    "        # 3) Optionally subsample chunks to limit training size\n",
    "        if max_chunks is not None and len(chunks) > max_chunks:\n",
    "            indices = np.random.choice(len(chunks), max_chunks, replace=False)\n",
    "            chunks = [chunks[i] for i in indices]\n",
    "\n",
    "        self.chunks = chunks\n",
    "        print(f\"[FullCloudChunkDataset] Total chunks: {len(self.chunks)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def _augment(self, xyz):\n",
    "        # rotation around Z + small jitter\n",
    "        xyz = xyz.copy()\n",
    "        theta = np.random.uniform(0, 2*np.pi)\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        R = np.array([[c, -s, 0],\n",
    "                      [s,  c, 0],\n",
    "                      [0,  0, 1]], dtype=np.float32)\n",
    "        xyz = xyz @ R.T\n",
    "        xyz += np.random.normal(0, 0.01, xyz.shape).astype(np.float32)\n",
    "        return xyz\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_str, start, end = self.chunks[idx]\n",
    "        xyz, lbl = self.clouds[path_str]\n",
    "\n",
    "        xyz = xyz[start:end]\n",
    "        lbl = lbl[start:end]\n",
    "\n",
    "        # center\n",
    "        xyz = xyz - xyz.mean(axis=0, keepdims=True)\n",
    "\n",
    "        if self.augment:\n",
    "            xyz = self._augment(xyz)\n",
    "\n",
    "        xyz_t = torch.from_numpy(xyz).float()\n",
    "        lbl_t = torch.from_numpy(lbl).long()\n",
    "        return xyz_t, lbl_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8d34b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 12  Val files: 4\n",
      "[FullCloudChunkDataset] Total chunks: 1500\n",
      "[FullCloudChunkDataset] Total chunks: 500\n",
      "train batches: 1500  val batches: 500\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "all_files = sorted([f.name for f in DATA_DIR.glob(\"*.ply\")])\n",
    "random.shuffle(all_files)\n",
    "\n",
    "N = len(all_files)\n",
    "train_files = all_files[:int(0.8 * N)]\n",
    "val_files   = all_files[int(0.8 * N):]\n",
    "\n",
    "print(\"Train files:\", len(train_files), \" Val files:\", len(val_files))\n",
    "\n",
    "train_ds_25 = FullCloudChunkDataset(\n",
    "    root=DATA_DIR,\n",
    "    files=train_files,\n",
    "    points_per_chunk=2048,  # smaller chunks\n",
    "    augment=True,\n",
    "    max_chunks=1500         # limit dataset\n",
    ")\n",
    "\n",
    "val_ds_25 = FullCloudChunkDataset(\n",
    "    root=DATA_DIR,\n",
    "    files=val_files,\n",
    "    points_per_chunk=2048,\n",
    "    augment=False,\n",
    "    max_chunks=500          # smaller eval set is fine\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds_25,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds_25,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \" val batches:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bcfc5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:03<00:00, 402.83it/s, loss=0.0955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss = 0.5933\n",
      "  Val Loss   = 0.6146\n",
      "  IoU        = [0.716 0.    0.   ]\n",
      "  mIoU       = 0.2387\n",
      "  âœ… Saved BEST model to ./checkpoints_kpconv_25nov/best_model_25nov_mIoU_0.2387.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:03<00:00, 401.98it/s, loss=0.24]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss = 0.2774\n",
      "  Val Loss   = 0.1805\n",
      "  IoU        = [0.94841457 0.         0.93355636]\n",
      "  mIoU       = 0.6273\n",
      "  âœ… Saved BEST model to ./checkpoints_kpconv_25nov/best_model_25nov_mIoU_0.6273.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:03<00:00, 408.26it/s, loss=0.0281]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss = 0.1428\n",
      "  Val Loss   = 0.1967\n",
      "  IoU        = [0.94252716 0.         0.85651973]\n",
      "  mIoU       = 0.5997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:03<00:00, 405.16it/s, loss=0.0466]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Train Loss = 0.1276\n",
      "  Val Loss   = 0.1378\n",
      "  IoU        = [0.95066486 0.         0.93419911]\n",
      "  mIoU       = 0.6283\n",
      "  âœ… Saved BEST model to ./checkpoints_kpconv_25nov/best_model_25nov_mIoU_0.6283.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:03<00:00, 402.77it/s, loss=0.0702]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Train Loss = 0.1224\n",
      "  Val Loss   = 0.1590\n",
      "  IoU        = [0.95123566 0.         0.92587028]\n",
      "  mIoU       = 0.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:03<00:00, 398.51it/s, loss=0.0117]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "  Train Loss = 0.1183\n",
      "  Val Loss   = 0.1627\n",
      "  IoU        = [0.95277164 0.         0.91191334]\n",
      "  mIoU       = 0.6216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:04<00:00, 373.82it/s, loss=0.0634]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "  Train Loss = 0.1209\n",
      "  Val Loss   = 0.1516\n",
      "  IoU        = [0.95284576 0.         0.89585277]\n",
      "  mIoU       = 0.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:03<00:00, 375.70it/s, loss=0.0126]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "  Train Loss = 0.1166\n",
      "  Val Loss   = 0.1624\n",
      "  IoU        = [0.95118846 0.         0.92877869]\n",
      "  mIoU       = 0.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:04<00:00, 374.57it/s, loss=0.0333]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "  Train Loss = 0.1151\n",
      "  Val Loss   = 0.2130\n",
      "  IoU        = [0.95238833 0.         0.86790318]\n",
      "  mIoU       = 0.6068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:04<00:00, 371.98it/s, loss=0.00654] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "  Train Loss = 0.1153\n",
      "  Val Loss   = 0.1813\n",
      "  IoU        = [0.95312301 0.         0.91109542]\n",
      "  mIoU       = 0.6214\n",
      "\n",
      "ðŸŽ‰ TRAINING COMPLETE!\n",
      "Best mIoU = 0.6282879931937195\n"
     ]
    }
   ],
   "source": [
    "#training loop new version\n",
    "# --- IMPORTS ---\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --- SETUP ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# âœ… NEW: separate folder for today\n",
    "SAVE_DIR = \"./checkpoints_kpconv_25nov\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# âœ… NEW: log file with 25nov suffix\n",
    "LOG_FILE = os.path.join(SAVE_DIR, \"training_log_25nov.csv\")\n",
    "\n",
    "# create log header\n",
    "with open(LOG_FILE, \"w\") as f:\n",
    "    f.write(\"epoch,train_loss,val_loss,iou0,iou1,iou2,miou\\n\")\n",
    "\n",
    "\n",
    "# --- IoU Function (same as before) ---\n",
    "def compute_iou(pred, gt, nc):\n",
    "    cm = confusion_matrix(gt, pred, labels=list(range(nc)))\n",
    "    ious = []\n",
    "    for i in range(nc):\n",
    "        tp = cm[i,i]\n",
    "        fp = cm[:,i].sum() - tp\n",
    "        fn = cm[i,:].sum() - tp\n",
    "        denom = tp + fp + fn\n",
    "        ious.append(tp / denom if denom > 0 else 0)\n",
    "    return np.array(ious)\n",
    "\n",
    "\n",
    "# --- MODEL, OPT, LOSS ---\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "best_miou = 0.0\n",
    "\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for ep in range(EPOCHS):\n",
    "\n",
    "    # ========== TRAIN ==========\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {ep+1}/{EPOCHS}\", leave=True)\n",
    "    total_train_loss = 0\n",
    "    batches = 0\n",
    "\n",
    "    for pts, lbl in loop:\n",
    "        pts, lbl = pts.to(DEVICE), lbl.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(pts)\n",
    "        loss = criterion(logits.view(-1,3), lbl.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        batches += 1\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_train_loss / batches\n",
    "\n",
    "    # ========== VALIDATION ==========\n",
    "    model.eval()\n",
    "    preds_all, lbl_all = [], []\n",
    "    total_val_loss = 0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pts, lbl in val_loader:\n",
    "            pts = pts.to(DEVICE)\n",
    "            lbl = lbl.to(DEVICE)\n",
    "            logits = model(pts)\n",
    "\n",
    "            loss = criterion(logits.view(-1,3), lbl.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "            pred = logits.argmax(-1).cpu().numpy().reshape(-1)\n",
    "            lbl_np = lbl.cpu().numpy().reshape(-1)\n",
    "\n",
    "            preds_all.append(pred)\n",
    "            lbl_all.append(lbl_np)\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_batches\n",
    "\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    lbl_all = np.concatenate(lbl_all)\n",
    "\n",
    "    iou = compute_iou(preds_all, lbl_all, 3)\n",
    "    miou = iou.mean()\n",
    "\n",
    "    print(f\"\\nEpoch {ep+1} Summary:\")\n",
    "    print(f\"  Train Loss = {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss   = {avg_val_loss:.4f}\")\n",
    "    print(f\"  IoU        = {iou}\")\n",
    "    print(f\"  mIoU       = {miou:.4f}\")\n",
    "\n",
    "    # ----- SAVE LOG -----\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{ep+1},{avg_train_loss:.4f},{avg_val_loss:.4f},\"\n",
    "            f\"{iou[0]:.4f},{iou[1]:.4f},{iou[2]:.4f},{miou:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "    # ----- SAVE BEST MODEL (with 25nov tag) -----\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        best_path = os.path.join(SAVE_DIR, f\"best_model_25nov_mIoU_{best_miou:.4f}.pth\")\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"  âœ… Saved BEST model to {best_path}\")\n",
    "\n",
    "    # ----- OPTIONAL: SAVE EVERY CHECKPOINT (with 25nov) -----\n",
    "    ckpt_path = os.path.join(SAVE_DIR, f\"checkpoint_epoch_{ep+1}_25nov.pth\")\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "print(\"\\nðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"Best mIoU =\", best_miou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from plyfile import PlyData\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "\n",
    "# ---------------------\n",
    "# Paths & model\n",
    "# ---------------------\n",
    "DATA_PATH = Path(\"/home/ccbd/Desktop/SSS_03/Data/train_sphere_ascii_roi\")\n",
    "ALL_FILES = sorted(DATA_PATH.glob(\"*.ply\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Files found:\", len(ALL_FILES))\n",
    "\n",
    "# âœ… use your new best model if you retrain:\n",
    "# best_ckpt_path = \"./checkpoints_kpconv_25nov/best_model_25nov_mIoU_0.xxxx.pth\"\n",
    "# or keep the old 24nov one for comparison:\n",
    "best_ckpt_path = \"./checkpoints_kpconv_24nov/best_model_24nov_mIoU_0.8541.pth\"\n",
    "\n",
    "best_model = KPNet(base=8, num_classes=3, K=6).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(best_ckpt_path, map_location=DEVICE))\n",
    "best_model.eval()\n",
    "print(\"Loaded:\", best_ckpt_path)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Helper: read GT and map labels\n",
    "# ---------------------------------------\n",
    "label_map = {1: 0, 3: 1, 9: 2}\n",
    "\n",
    "def read_gt_labels(path):\n",
    "    pd = PlyData.read(str(path))\n",
    "    v = pd[\"vertex\"].data\n",
    "\n",
    "    lbl = np.array(v[\"scalar_NewClassification\"]).astype(np.int64)\n",
    "    lbl = np.vectorize(lambda x: label_map.get(int(x), 255))(lbl)\n",
    "\n",
    "    mask = lbl != 255\n",
    "    return mask, lbl[mask]\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Helper: run model on full cloud (chunked)\n",
    "# ---------------------------------------\n",
    "def predict_xyz(model, xyz, chunk=4096):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(xyz), chunk):\n",
    "            part = xyz[i:i+chunk]\n",
    "            part = torch.from_numpy(part).float().unsqueeze(0).to(DEVICE)\n",
    "            logits = model(part)\n",
    "            pred = logits.argmax(-1).squeeze(0).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN LOOP\n",
    "# ---------------------------------------\n",
    "results = []\n",
    "\n",
    "for path in ALL_FILES:\n",
    "    print(f\"\\nProcessing tile: {path.name}\")\n",
    "\n",
    "    pd = PlyData.read(str(path))\n",
    "    v = pd[\"vertex\"].data\n",
    "    xyz = np.vstack([v[\"x\"], v[\"y\"], v[\"z\"]]).T.astype(np.float32)\n",
    "\n",
    "    valid_mask, gt = read_gt_labels(path)\n",
    "    xyz_valid = xyz[valid_mask]\n",
    "\n",
    "    # ðŸ”¹ Keep a similar normalization scheme (optional, but safe & consistent)\n",
    "    xyz_valid = xyz_valid - xyz_valid.mean(axis=0, keepdims=True)\n",
    "\n",
    "    pred = predict_xyz(best_model, xyz_valid)\n",
    "\n",
    "    cm = confusion_matrix(gt, pred, labels=[0,1,2])\n",
    "\n",
    "    ious = []\n",
    "    for c in range(3):\n",
    "        tp = cm[c,c]\n",
    "        fp = cm[:,c].sum() - tp\n",
    "        fn = cm[c,:].sum() - tp\n",
    "        denom = tp + fp + fn\n",
    "        iou = tp/denom if denom > 0 else 0\n",
    "        ious.append(iou)\n",
    "\n",
    "    miou = float(np.mean(ious))\n",
    "    print(\"IoUs:\", ious, \"   mIoU =\", miou)\n",
    "\n",
    "    results.append([path.name] + ious + [miou])\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# âœ… Save CSV in a 25nov-specific folder\n",
    "# ---------------------------------------\n",
    "EVAL_DIR = \"./eval_fulltiles_25nov\"\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "\n",
    "csv_path = os.path.join(EVAL_DIR, \"full_tile_results_25nov.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"filename\", \"iou0\", \"iou1\", \"iou2\", \"miou\"])\n",
    "    w.writerows(results)\n",
    "\n",
    "print(\"\\nSaved:\", csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
